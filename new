import numpy as np
import math
import csv
import pandas as pd
import matplotlib.pyplot as plt
import mplcursors
from scipy.stats import chi2

# Define lists to store results
r = []
el = []
az = []

class CVFilter:
    def __init__(self):
        self.Sf = np.zeros((6, 1))  # Filter state vector
        self.Pf = np.eye(6)  # Filter state covariance matrix
        self.Sp = np.zeros((6, 1))  # Predicted state vector
        self.Pp = np.eye(6)  # Predicted state covariance matrix
        self.plant_noise = 20  # Plant noise covariance
        self.H = np.eye(3, 6)  # Measurement matrix
        self.R = np.eye(3)  # Measurement noise covariance
        self.Meas_Time = 0  # Measured time
        self.prev_Time = 0
        self.Q = np.eye(6)
        self.Phi = np.eye(6)
        self.Z = np.zeros((3, 1)) 
        self.Z1 = np.zeros((3, 1)) # Measurement vector
        self.Z2 = np.zeros((3, 1)) 
        self.first_rep_flag = False
        self.second_rep_flag = False
        self.gate_threshold = 9000.21  # 95% confidence interval for Chi-square distribution with 3 degrees of freedom

    def initialize_filter_state(self, x, y, z, time):
        if not self.first_rep_flag:
            # First measurement, initialize positions
            self.Z1 = np.array([[x], [y], [z]])
            self.Sf[0] = x
            self.Sf[1] = y
            self.Sf[2] = z
            self.Meas_Time = time
            self.prev_Time = self.Meas_Time
            self.first_rep_flag = True
        elif self.first_rep_flag and not self.second_rep_flag:
            # Second measurement, initialize velocities
            self.Z2 = np.array([[x], [y], [z]])
            dt = time - self.prev_Time
            self.Sf[3] = (self.Z2[0] - self.Z1[0]) / dt  # vx
            self.Sf[4] = (self.Z2[1] - self.Z1[1]) / dt  # vy
            self.Sf[5] = (self.Z2[2] - self.Z1[2]) / dt  # vz
            self.prev_Time = self.Meas_Time
            self.Meas_Time = time
            self.second_rep_flag = True
        else:
            # For subsequent measurements
            self.Z = np.array([[x], [y], [z]])
            self.prev_Time = self.Meas_Time
            self.Meas_Time = time

    def predict_step(self, current_time):
        dt = current_time - self.prev_Time
        self.Phi = np.eye(6)
        self.Phi[0, 3] = dt
        self.Phi[1, 4] = dt
        self.Phi[2, 5] = dt
        
        # Process noise matrix (Q)
        T_2 = (dt * dt) / 2.0
        T_3 = (dt * dt * dt) / 3.0
        self.Q = np.zeros((6, 6))
        self.Q[0, 0] = self.Q[1, 1] = self.Q[2, 2] = T_3
        self.Q[0, 3] = self.Q[1, 4] = self.Q[2, 5] = T_2
        self.Q[3, 0] = self.Q[4, 1] = self.Q[5, 2] = T_2
        self.Q[3, 3] = self.Q[4, 4] = self.Q[5, 5] = dt
        self.Q = self.Q * self.plant_noise

        self.Sp = np.dot(self.Phi, self.Sf)
        self.Pp = np.dot(np.dot(self.Phi, self.Pf), self.Phi.T) + self.Q
        self.Meas_Time = current_time

    def update_step(self, Z):
        Inn = Z - np.dot(self.H, self.Sp)  # Innovation
        S = np.dot(self.H, np.dot(self.Pp, self.H.T)) + self.R  # Innovation covariance
        K = np.dot(np.dot(self.Pp, self.H.T), np.linalg.inv(S))  # Kalman gain
        self.Sf = self.Sp + np.dot(K, Inn)  # Update state estimate
        self.Pf = np.dot(np.eye(6) - np.dot(K, self.H), self.Pp)  # Update estimate covariance

def read_measurements_from_csv(file_path):
    measurements = []
    with open(file_path, 'r') as file:
        reader = csv.reader(file)
        next(reader)  # Skip header if exists
        for row in reader:
            # Adjust column indices based on CSV file structure
            mr = float(row[7])  # MR column
            ma = float(row[8])  # MA column
            me = float(row[9])  # ME column
            mt = float(row[10])  # MT column
            x, y, z = sph2cart(ma, me, mr)  # Convert spherical to Cartesian coordinates
            r, az, el = cart2sph(x, y, z)  # Convert Cartesian to spherical coordinates
            measurements.append((r, az, el, mt))
    return measurements

def sph2cart(az, el, r):
    x = r * np.cos(el * np.pi / 180) * np.sin(az * np.pi / 180)
    y = r * np.cos(el * np.pi / 180) * np.cos(az * np.pi / 180)
    z = r * np.sin(el * np.pi / 180)
    return x, y, z

def cart2sph(x, y, z):
    r = np.sqrt(x**2 + y**2 + z**2)
    el = math.atan2(z, np.sqrt(x**2 + y**2)) * 180 / np.pi
    az = math.atan2(y, x)

    if x > 0.0:
        az = np.pi / 2 - az
    else:
        az = 3 * np.pi / 2 - az

    az = az * 180 / np.pi

    if az < 0.0:
        az = 360 + az

    if az > 360:
        az = az - 360

    return r, az, el

def cart2sph2(x: float, y: float, z: float, filtered_values_csv):
    for i in range(len(filtered_values_csv)):
        r.append(np.sqrt(x[i]**2 + y[i]**2 + z[i]**2))
        el.append(math.atan(z[i] / np.sqrt(x[i]**2 + y[i]**2)) * 180 / 3.14)
        az.append(math.atan(y[i] / x[i]))

        if x[i] > 0.0:                
            az[i] = 3.14 / 2 - az[i]
        else:
            az[i] = 3 * 3.14 / 2 - az[i]       

        az[i] = az[i] * 180 / 3.14 

        if az[i] < 0.0:
            az[i] = 360 + az[i]

        if az[i] > 360:
            az[i] = az[i] - 360   
      
    return r, az, el

def form_measurement_groups(measurements, max_time_diff=0.050):
    measurement_groups = []
    current_group = []
    base_time = measurements[0][3]

    for measurement in measurements:
        if measurement[3] - base_time <= max_time_diff:
            current_group.append(measurement)
        else:
            measurement_groups.append(current_group)
            current_group = [measurement]
            base_time = measurement[3]

    if current_group:
        measurement_groups.append(current_group)

    return measurement_groups

def generate_hypotheses(tracks, reports):
    hypotheses = []
    for i, track in enumerate(tracks):
        for j, report in enumerate(reports):
            if i == j:
                hypotheses.append([(i, j)])
    return hypotheses

def calculate_probabilities(hypotheses, tracks, reports, cov_inv):
    probabilities = []
    for hypothesis in hypotheses:
        prob = 1.0
        for track_idx, report_idx in hypothesis:
            track = tracks[track_idx]
            report = reports[report_idx]
            diff = report - track
            prob *= np.exp(-0.5 * np.dot(diff.T, np.dot(cov_inv, diff)))
        probabilities.append(prob)
    probabilities = np.array(probabilities)
    probabilities /= np.sum(probabilities)
    return probabilities

def get_association_weights(hypotheses, probabilities):
    association_weights = {}
    for hypothesis, prob in zip(hypotheses, probabilities):
        for track_idx, report_idx in hypothesis:
            association_weights[(track_idx, report_idx)] = prob
    return association_weights

def mahalanobis_distance(report, track, cov_inv):
    diff = report - track
    return np.sqrt(np.dot(diff.T, np.dot(cov_inv, diff)))

def form_clusters_via_association(tracks, reports, cov_inv, gate_threshold):
    clusters = []
    associated_tracks = set()
    associated_reports = set()

    for i, track in enumerate(tracks):
        cluster = {'track': i, 'reports': []}
        for j, report in enumerate(reports):
            dist = mahalanobis_distance(report, track, cov_inv)
            if dist < gate_threshold:
                cluster['reports'].append(j)
                associated_reports.add(j)
        if cluster['reports']:
            clusters.append(cluster)
            associated_tracks.add(i)

    unassociated_reports = [j for j in range(len(reports)) if j not in associated_reports]

    return clusters, list(associated_tracks), unassociated_reports

def main():
    file_path = "measurements.csv"  # Adjust the path to your CSV file
    measurements = read_measurements_from_csv(file_path)

    measurement_groups = form_measurement_groups(measurements)

    kalman_filter = CVFilter()
    track_list = []

    for i, group in enumerate(measurement_groups):
        current_time = group[0][3]
        kalman_filter.predict_step(current_time)
        
        tracks = []
        reports = []
        for measurement in group:
            r, az, el, mt = measurement
            x, y, z = sph2cart(az, el, r)
            if not kalman_filter.second_rep_flag:
                kalman_filter.initialize_filter_state(x, y, z, mt)
            else:
                Z = np.array([[x], [y], [z]])
                reports.append(Z)

            tracks.append(kalman_filter.Sp[:3])

        tracks = np.array(tracks)
        reports = np.array(reports)
        cov_inv = np.linalg.inv(kalman_filter.Pp[:3, :3])

        hypotheses = generate_hypotheses(tracks, reports)
        probabilities = calculate_probabilities(hypotheses, tracks, reports, cov_inv)
        association_weights = get_association_weights(hypotheses, probabilities)

        clusters, associated_tracks, unassociated_reports = form_clusters_via_association(
            tracks, reports, cov_inv, kalman_filter.gate_threshold)

        best_hypothesis_index = np.argmax(probabilities)
        best_hypothesis = hypotheses[best_hypothesis_index]

        for track_idx, report_idx in best_hypothesis:
            kalman_filter.update_step(reports[report_idx])

    filtered_values_csv = pd.read_csv("filtered_values.csv")

    cart2sph2(filtered_values_csv['x'], filtered_values_csv['y'], filtered_values_csv['z'], filtered_values_csv)
    plt.plot(r, az)
    plt.xlabel("Range (meters)")
    plt.ylabel("Azimuth (degrees)")
    plt.title("Filtered Values")
    mplcursors.cursor()
    plt.show()

if __name__ == "__main__":
    main()
